#!/usr/bin/env ansible-playbook

- name: Provision docker CI containers.
  hosts: localhost
  become: no
  gather_facts: true
  tasks:
    - name: Pull in systemd-enabled docker base image.
      docker_image:
        name: "{{ docker_image_sd_build }}"
      when: "{{ docker_instances != [] }}"
      tags: docker

    - name: Start Tor docker proxy for use in functional app tests.
      docker_container:
        name: tor
        image: "hsaito/torbox"
        state: started
      when: "{{ docker_instances != [] }}"
      tags: docker

    - name: Start CI-specific build containers.
      docker_container:
        name: "{{ item }}"
        hostname: "{{ item }}"
        image: "{{ docker_image_sd_build }}"
        command: /lib/systemd/systemd
        devices:
          - "/sys/fs/cgroup:/sys/fs/cgroup:ro"
        privileged: true
        env:
          TOR_PROXY: "{{ lookup('pipe', 'docker exec tor hostname -I') }}"
      with_items: "{{ docker_instances }}"
      tags: docker
  vars:
    ci_environment: "{{ ansible_env.CI_SD_ENV | mandatory }}"
    docker_image_sd_build: "msheiny/debian-jessie-systemd"
  vars_files:
    - "../vars/{{ ci_environment }}.yml"

- name: Create CI remote hosts for testing SecureDrop config.
  hosts: localhost
  become: no
  gather_facts: false
  tasks:
    - name: Ensure CI environment vars are declared.
      assert:
        that:
          - ci_environment != ""
          - aws_ec2_ci_tags.build_num != ""
          - aws_ec2_vpc_id != ""

    - name: Store job_id (job name and build number) as host fact.
      set_fact:
        job_id: "{{ aws_ec2_ci_tags.job_name }}-{{ aws_ec2_ci_tags.build_num }}"

    - name: Generate temporary SSH key for logging into CI hosts.
      user:
        name: "{{ ansible_user_id }}"
        generate_ssh_key: yes
        ssh_key_bits: 4096
        ssh_key_file: "{{ ansible_user_dir }}/.ssh/{{ job_id }}"

    - name: Add temporary SSH key to AWS region.
      ec2_key:
        name: "sdci-{{ job_id }}"
        region: "{{ aws_ec2_ci_region }}"
        key_material: "{{ lookup('file',ansible_user_dir+'/.ssh/{{ job_id }}.pub') }}"

    - name: Find Ubuntu AMI.
      ec2_ami_find:
        owner: 099720109477
        name: "ubuntu/images/hvm/ubuntu-trusty-14.04-amd64-server*"
        region: "{{ aws_ec2_ci_region }}"
        sort: creationDate
        sort_order: descending
        sort_end: 1
        virtualization_type: hvm
      register: ami_search_result

    - name: Find VPC subnet.
      ec2_vpc_subnet_facts:
        region: "{{ aws_ec2_ci_region }}"
        filters:
          vpc-id: "{{ aws_ec2_vpc_id }}"
      register: vpc_subnet_result

    - name: Create custom temporary security_group for CI hosts.
      ec2_group:
        name: "{{ job_id }}"
        description: Temporary rules for CI
        region: "{{ aws_ec2_ci_region }}"
        vpc_id: "{{ aws_ec2_vpc_id }}"
        rules:
          - proto: tcp
            from_port: 22
            to_port: 22
            cidr_ip: "{{ ci_box_ip  }}/32"
          - proto: tcp
            from_port: 1514
            to_port: 1515
            cidr_ip: "{{ vpc_subnet_result.subnets[0].cidr_block }}"
          - proto: udp
            from_port: 1514
            to_port: 1515
            cidr_ip: "{{ vpc_subnet_result.subnets[0].cidr_block }}"
      register: group_create_result

    - name: Create remote CI hosts on AWS.
      ec2:
        group_id: "{{ group_create_result.group_id }}"
        vpc_subnet_id: "{{ vpc_subnet_result.subnets[0].id }}"
        assign_public_ip: yes
        image: "{{ ami_search_result.results[0].ami_id }}"
        region: "{{ aws_ec2_ci_region }}"
        key_name: "sdci-{{ job_id }}"
        exact_count: 1
        instance_type: "{{ aws_ec2_instance_type }}"
        instance_initiated_shutdown_behavior: stop
        wait: True
        user_data: "{{ aws_ec2_ci_userdata }}"
        volumes:
          - device_name: /dev/xvda
            volume_type: gp2
            volume_size: 15
            delete_on_termination: True
        instance_tags: "{{ aws_ec2_ci_tags | combine(item) }}"
        count_tag: "{{ aws_ec2_ci_tags | combine(item) }}"
        instance_profile_name: securedrop
      register: reg_ec_instance
      with_items: "{{ aws_instance_tags }}"

    - name: Wait for EC2 instances to boot.
      wait_for:
        host: "{{ item.tagged_instances[0]['public_dns_name'] }}"
        port: 22
        timeout: "{{ aws_ec2_boot_timeout }}"
        state: started
      with_items: "{{ reg_ec_instance.results }}"
      when: "item.tagged_instances[0].tags['build_num'] == aws_ec2_ci_tags.build_num"

    - name: Add AWS hosts to temporary local SSH config.
      template:
        dest: "{{ ansible_user_dir }}/.ssh/sshconfig-{{ job_id }}"
        src: ../templates/ssh_config
  vars:
    aws_ec2_instance_type: "{{ ansible_env.CI_AWS_TYPE | mandatory }}"
    aws_ec2_ci_region: "{{ ansible_env.CI_AWS_REGION | default('us-west-1') }}"
    aws_ec2_vpc_id: "{{ ansible_env.CI_AWS_VPC_ID | mandatory }}"
    aws_ec2_boot_timeout: "500"
    ci_environment: "{{ ansible_env.CI_SD_ENV | mandatory }}"
    ci_box_ip: "{{ lookup('pipe', 'curl -s ifconfig.co') }}"
    aws_ec2_ci_tags:
      job_name: "securedrop-ci"
      build_num: "{{ ansible_env.BUILD_NUM | mandatory }}"
    aws_ec2_ci_userdata: |
      #!/bin/bash
      adduser --ingroup sudo --disabled-password --gecos "" {{ ansible_user_id }}
      mkdir -p /home/{{ ansible_user_id }}/.ssh
      curl -s http://169.254.169.254/2011-01-01/meta-data/public-keys/0/openssh-key > /home/{{ ansible_user_id }}/.ssh/authorized_keys
      chown {{ ansible_user_id }} -R /home/{{ ansible_user_id }}/.ssh
      chmod 700 -R /home/{{ ansible_user_id }}/.ssh
      echo "{{ ansible_user_id }} ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers
  vars_files:
    - "../vars/{{ ci_environment }}.yml"
